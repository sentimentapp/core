{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VA-Spacial CNN-LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNIDa3i8Hw2q4/sY+ln4/1X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bK5c_cFxALO"
      },
      "source": [
        "## Dimensional Sentiment Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ot94gaezEUx"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook seeks to implement a variation on the VA-spacial, dimensional CNN-LSTM for sentiment analysis based on ([Wang et al. 2016](https://www.aclweb.org/anthology/P16-2037.pdf); [Wang et al. 2020](https://ieeexplore.ieee.org/ielx7/6570655/8938144/08930925.pdf)).\n",
        "\n",
        "The dataset used is JULIELab's [EmoBank](https://github.com/JULIELab/EmoBank), a large-scale (10k sentence) VAP-scheme corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEq9nXwAzNrB"
      },
      "source": [
        "## Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkA6MMJjzUDD"
      },
      "source": [
        "### Download Data\n",
        "EmoBank provides three prominent datasets: 1) the reader perspective, 2) the writer perspective, and 3) the weighted average of reader and writer annotations (`emobank.csv`; we'll use this one)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2upnj0vv1u1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "c3e7818f-d34c-422d-d02e-353f2a0416f3"
      },
      "source": [
        "!wget --show-progress --continue -O /content/emobank.csv https://raw.githubusercontent.com/JULIELab/EmoBank/master/corpus/emobank.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-22 16:24:51--  https://raw.githubusercontent.com/JULIELab/EmoBank/master/corpus/emobank.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1335010 (1.3M) [text/plain]\n",
            "Saving to: ‘/content/emobank.csv’\n",
            "\n",
            "/content/emobank.cs 100%[===================>]   1.27M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-07-22 16:24:51 (9.86 MB/s) - ‘/content/emobank.csv’ saved [1335010/1335010]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dnH83MjiH7T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "36aac393-9658-4b70-e772-e1ba8168ce9d"
      },
      "source": [
        "!wget --show-progress --continue -O /content/crawl-300d-50K.vec.zip https://mb-14.github.io/static/crawl-300d-50K.vec.zip\n",
        "!unzip /content/crawl-300d-50K.vec.zip -d /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-22 16:24:54--  https://mb-14.github.io/static/crawl-300d-50K.vec.zip\n",
            "Resolving mb-14.github.io (mb-14.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to mb-14.github.io (mb-14.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37233955 (36M) [application/zip]\n",
            "Saving to: ‘/content/crawl-300d-50K.vec.zip’\n",
            "\n",
            "/content/crawl-300d 100%[===================>]  35.51M  51.4MB/s    in 0.7s    \n",
            "\n",
            "2020-07-22 16:24:55 (51.4 MB/s) - ‘/content/crawl-300d-50K.vec.zip’ saved [37233955/37233955]\n",
            "\n",
            "Archive:  /content/crawl-300d-50K.vec.zip\n",
            "  inflating: /content/crawl-300d-50K.vec  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6KQ7OfS5Sdc"
      },
      "source": [
        "### Loading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ia365IQ5Rh0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "2b62a7d8-7b32-4ee0-f02d-15a8169b80cc"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
        "  raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
        "\n",
        "full = pd.read_csv(\"/content/emobank.csv\", index_col=0)[['split', 'text', 'V', 'A']]\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sanitize = lambda s: [i.lower().translate(str.maketrans('', '', '.?!.;:()[]/')) for i in nltk.tokenize.sent_tokenize(s)]\n",
        "\n",
        "# sanitize = lambda s: s.lower().translate(str.maketrans('', '', '.?!.;:()[]/'))\n",
        "\n",
        "full['text'] = full['text'].map(sanitize)\n",
        "\n",
        "train = full[full['split'] == 'train'][['text', 'V', 'A']]\n",
        "test = full[full['split'] == 'test'][['text', 'V', 'A']]\n",
        "\n",
        "print(train.sort_values(by=['V']))\n",
        "\n",
        "len(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "                                                                                     text  ...     A\n",
            "id                                                                                         ...      \n",
            "A_defense_of_Michael_Moore_12034_12044                                       [\"fuck you\"]  ...  4.20\n",
            "captured_moments_5506_5538                              [i hate it, despise it, abhor it]  ...  4.40\n",
            "captured_moments_6594_6611                                            [\"obscenely ugly,\"]  ...  2.90\n",
            "detroit_13623_13627                                                                 [sad]  ...  3.20\n",
            "Nathans_Bylichka_7597_7686              [my girlfriend has disappeared, i don’t even k...  ...  4.00\n",
            "...                                                                                   ...  ...   ...\n",
            "captured_moments_28753_28863            [for a perfect moment, emil and tasha and i we...  ...  3.88\n",
            "Madame_White_Snake_7985_8080            [“i searched everywhere, and now that at last ...  ...  3.62\n",
            "captured_moments_33365_33387                                      [\"tell her i love her\"]  ...  4.10\n",
            "20020731-nyt_25143_25174                                 [\"i am thrilled with the price\"]  ...  4.00\n",
            "vampires_4446_4474                                          [lol wonderful simply superb]  ...  4.30\n",
            "\n",
            "[8062 rows x 3 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L7iGIQ-OfmJ"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VViSVQl1Oj_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "129819b2-3bdb-4ac1-e95b-8d33a0e528a4"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH=50\n",
        "MAX_SENTENCE_LENGTH=70\n",
        "MAX_ENTRY_LENGTH=15\n",
        "MAX_NUMBER_WORDS=50000\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUMBER_WORDS)\n",
        "sentences = flatten(train['text'].values)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "def tokenize_and_pad(dataset):\n",
        "  sequences = [tokenizer.texts_to_sequences(i) for i in dataset['text'].values]\n",
        "  x_unpadded = np.array([pad_sequences(seq, maxlen=MAX_SENTENCE_LENGTH) for seq in sequences])\n",
        "\n",
        "  def pad_3d_array(array, shape):\n",
        "    padded = np.zeros(shape, dtype=int)\n",
        "    for x in range(shape[0]):\n",
        "      align_bottom_index = shape[1] - len(array[x])\n",
        "      end_index = shape[1]-1\n",
        "      if align_bottom_index == end_index:\n",
        "        padded[x][end_index] = array[x]\n",
        "      else:\n",
        "        padded[x][align_bottom_index:end_index] = array[x][0:len(array[x])-1]\n",
        "    return padded\n",
        "\n",
        "  x_f = pad_3d_array(x_unpadded, (len(dataset['text'].values), MAX_ENTRY_LENGTH, MAX_SENTENCE_LENGTH))\n",
        "  y_f = np.array(dataset[['V', 'A']].values)\n",
        "\n",
        "  return x_f, y_f\n",
        "\n",
        "x_train, y_train = tokenize_and_pad(train)\n",
        "x_val, y_val = tokenize_and_pad(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSC-h2xNVnL"
      },
      "source": [
        "## Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4UPoksdg43Z"
      },
      "source": [
        "### Make Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nk206OYg4qA"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "def make_embedding_layer(word_index, embeddings_path):\n",
        "    embeddings = KeyedVectors.load_word2vec_format(embeddings_path)\n",
        "    embedding_dims = embeddings.vector_size\n",
        "    nb_words = min(len(embeddings.vocab), len(word_index))+1\n",
        "\n",
        "    embedding_matrix = np.zeros((nb_words, embedding_dims))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i >= nb_words:\n",
        "            continue\n",
        "        try:\n",
        "            embedding_vector = embeddings.get_vector(word)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    embedding_layer = layers.Embedding(input_dim=nb_words, output_dim=embedding_dims, embeddings_initializer=Constant(embedding_matrix),\n",
        "                                input_length=MAX_SENTENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "    return embedding_layer, embedding_dims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPhwOkJc9xf5"
      },
      "source": [
        "### Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3GILfcy93TX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "4f0a3ecb-96b2-41fe-a154-e6ce9ae3c57b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "\n",
        "CNN_DIM = 32\n",
        "CNN_WINDOW = 3\n",
        "POOL_SIZE = 2\n",
        "\n",
        "def cnn_lstm_model(word_index, embeddings, seq_len=100, batch_size=300, stateful=True):\n",
        "  embedded_sequences, embeddings_dim = make_embedding_layer(word_index, embeddings)\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "    layers.TimeDistributed(embedded_sequences, input_shape=(MAX_ENTRY_LENGTH, MAX_SENTENCE_LENGTH)),\n",
        "    layers.TimeDistributed(layers.Conv1D(CNN_DIM, CNN_WINDOW, activation='relu')),\n",
        "    layers.TimeDistributed(layers.MaxPool1D(pool_size=POOL_SIZE, strides=1, padding='valid')),\n",
        "    layers.TimeDistributed(layers.Flatten()),\n",
        "    layers.LSTM(20),\n",
        "    layers.Dense(10),\n",
        "    layers.Dense(2)\n",
        "  ])\n",
        "\n",
        "model = cnn_lstm_model(word_index, '/content/crawl-300d-50K.vec')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_3 (TimeDist (None, 15, 70, 300)       4682400   \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 15, 68, 32)        28832     \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 15, 67, 32)        0         \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 15, 2144)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20)                173200    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 4,884,664\n",
            "Trainable params: 202,264\n",
            "Non-trainable params: 4,682,400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0DM354sNbmO"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlv6_8h0aiCS"
      },
      "source": [
        "### Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ252bOtaqSC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2227540d-7f18-4d1d-bde3-0b0ca4eb4e04"
      },
      "source": [
        "model = cnn_lstm_model(word_index, '/content/crawl-300d-50K.vec')\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
        "model.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), epochs=50, batch_size=100)\n",
        "\n",
        "model.save('Dimensional_Sentiment_Model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "81/81 [==============================] - 57s 707ms/step - loss: 0.9545 - acc: 0.4895 - val_loss: 0.0969 - val_acc: 0.4680\n",
            "Epoch 2/50\n",
            "81/81 [==============================] - 56s 692ms/step - loss: 0.0983 - acc: 0.4392 - val_loss: 0.0907 - val_acc: 0.4250\n",
            "Epoch 3/50\n",
            "81/81 [==============================] - 56s 697ms/step - loss: 0.0950 - acc: 0.4184 - val_loss: 0.0890 - val_acc: 0.4090\n",
            "Epoch 4/50\n",
            "81/81 [==============================] - 57s 702ms/step - loss: 0.0922 - acc: 0.4272 - val_loss: 0.0850 - val_acc: 0.4010\n",
            "Epoch 5/50\n",
            "81/81 [==============================] - 56s 695ms/step - loss: 0.0836 - acc: 0.5459 - val_loss: 0.0787 - val_acc: 0.5310\n",
            "Epoch 6/50\n",
            "81/81 [==============================] - 56s 692ms/step - loss: 0.0763 - acc: 0.6118 - val_loss: 0.0755 - val_acc: 0.6110\n",
            "Epoch 7/50\n",
            "81/81 [==============================] - 56s 689ms/step - loss: 0.0718 - acc: 0.6524 - val_loss: 0.0734 - val_acc: 0.6140\n",
            "Epoch 8/50\n",
            "81/81 [==============================] - 56s 695ms/step - loss: 0.0687 - acc: 0.6770 - val_loss: 0.0732 - val_acc: 0.6580\n",
            "Epoch 9/50\n",
            "81/81 [==============================] - 56s 697ms/step - loss: 0.0651 - acc: 0.6961 - val_loss: 0.0724 - val_acc: 0.5940\n",
            "Epoch 10/50\n",
            "81/81 [==============================] - 56s 695ms/step - loss: 0.0621 - acc: 0.7079 - val_loss: 0.0724 - val_acc: 0.6750\n",
            "Epoch 11/50\n",
            "81/81 [==============================] - 56s 694ms/step - loss: 0.0590 - acc: 0.7148 - val_loss: 0.0723 - val_acc: 0.6870\n",
            "Epoch 12/50\n",
            "81/81 [==============================] - 56s 696ms/step - loss: 0.0558 - acc: 0.7326 - val_loss: 0.0742 - val_acc: 0.6800\n",
            "Epoch 13/50\n",
            "81/81 [==============================] - 56s 688ms/step - loss: 0.0531 - acc: 0.7316 - val_loss: 0.0735 - val_acc: 0.6460\n",
            "Epoch 14/50\n",
            "81/81 [==============================] - 55s 681ms/step - loss: 0.0499 - acc: 0.7388 - val_loss: 0.0739 - val_acc: 0.6710\n",
            "Epoch 15/50\n",
            "81/81 [==============================] - 56s 686ms/step - loss: 0.0468 - acc: 0.7461 - val_loss: 0.0761 - val_acc: 0.6680\n",
            "Epoch 16/50\n",
            "81/81 [==============================] - 56s 688ms/step - loss: 0.0440 - acc: 0.7530 - val_loss: 0.0771 - val_acc: 0.6460\n",
            "Epoch 17/50\n",
            "81/81 [==============================] - 56s 686ms/step - loss: 0.0414 - acc: 0.7561 - val_loss: 0.0783 - val_acc: 0.6750\n",
            "Epoch 18/50\n",
            "81/81 [==============================] - 56s 690ms/step - loss: 0.0390 - acc: 0.7628 - val_loss: 0.0804 - val_acc: 0.6390\n",
            "Epoch 19/50\n",
            "81/81 [==============================] - 56s 695ms/step - loss: 0.0364 - acc: 0.7728 - val_loss: 0.0817 - val_acc: 0.6470\n",
            "Epoch 20/50\n",
            "81/81 [==============================] - 56s 697ms/step - loss: 0.0334 - acc: 0.7824 - val_loss: 0.0835 - val_acc: 0.6510\n",
            "Epoch 21/50\n",
            "81/81 [==============================] - 56s 689ms/step - loss: 0.0312 - acc: 0.7895 - val_loss: 0.0845 - val_acc: 0.6720\n",
            "Epoch 22/50\n",
            "81/81 [==============================] - 56s 691ms/step - loss: 0.0293 - acc: 0.7972 - val_loss: 0.0869 - val_acc: 0.6510\n",
            "Epoch 23/50\n",
            "81/81 [==============================] - 55s 680ms/step - loss: 0.0273 - acc: 0.8046 - val_loss: 0.0866 - val_acc: 0.6560\n",
            "Epoch 24/50\n",
            "81/81 [==============================] - 56s 697ms/step - loss: 0.0257 - acc: 0.8075 - val_loss: 0.0881 - val_acc: 0.6600\n",
            "Epoch 25/50\n",
            "81/81 [==============================] - 56s 689ms/step - loss: 0.0238 - acc: 0.8090 - val_loss: 0.0908 - val_acc: 0.6680\n",
            "Epoch 26/50\n",
            "81/81 [==============================] - 56s 695ms/step - loss: 0.0226 - acc: 0.8190 - val_loss: 0.0948 - val_acc: 0.6650\n",
            "Epoch 27/50\n",
            "81/81 [==============================] - 56s 691ms/step - loss: 0.0212 - acc: 0.8257 - val_loss: 0.0935 - val_acc: 0.6610\n",
            "Epoch 28/50\n",
            "81/81 [==============================] - 56s 692ms/step - loss: 0.0202 - acc: 0.8324 - val_loss: 0.0965 - val_acc: 0.6600\n",
            "Epoch 29/50\n",
            "81/81 [==============================] - 55s 683ms/step - loss: 0.0187 - acc: 0.8350 - val_loss: 0.0959 - val_acc: 0.6340\n",
            "Epoch 30/50\n",
            "81/81 [==============================] - 55s 675ms/step - loss: 0.0178 - acc: 0.8421 - val_loss: 0.0980 - val_acc: 0.6390\n",
            "Epoch 31/50\n",
            "81/81 [==============================] - 55s 679ms/step - loss: 0.0167 - acc: 0.8452 - val_loss: 0.1000 - val_acc: 0.6500\n",
            "Epoch 32/50\n",
            "81/81 [==============================] - 55s 676ms/step - loss: 0.0160 - acc: 0.8484 - val_loss: 0.1016 - val_acc: 0.6520\n",
            "Epoch 33/50\n",
            "81/81 [==============================] - 54s 669ms/step - loss: 0.0150 - acc: 0.8539 - val_loss: 0.1040 - val_acc: 0.6310\n",
            "Epoch 34/50\n",
            "81/81 [==============================] - 53s 660ms/step - loss: 0.0146 - acc: 0.8595 - val_loss: 0.1040 - val_acc: 0.6380\n",
            "Epoch 35/50\n",
            "81/81 [==============================] - 51s 631ms/step - loss: 0.0137 - acc: 0.8613 - val_loss: 0.1048 - val_acc: 0.6380\n",
            "Epoch 36/50\n",
            "81/81 [==============================] - 61s 750ms/step - loss: 0.0131 - acc: 0.8680 - val_loss: 0.1060 - val_acc: 0.6410\n",
            "Epoch 37/50\n",
            "81/81 [==============================] - 62s 761ms/step - loss: 0.0130 - acc: 0.8660 - val_loss: 0.1067 - val_acc: 0.6450\n",
            "Epoch 38/50\n",
            "81/81 [==============================] - 61s 752ms/step - loss: 0.0126 - acc: 0.8650 - val_loss: 0.1086 - val_acc: 0.6570\n",
            "Epoch 39/50\n",
            "81/81 [==============================] - 61s 757ms/step - loss: 0.0114 - acc: 0.8758 - val_loss: 0.1080 - val_acc: 0.6400\n",
            "Epoch 40/50\n",
            "81/81 [==============================] - 62s 761ms/step - loss: 0.0111 - acc: 0.8772 - val_loss: 0.1104 - val_acc: 0.6290\n",
            "Epoch 41/50\n",
            "81/81 [==============================] - 61s 755ms/step - loss: 0.0105 - acc: 0.8799 - val_loss: 0.1120 - val_acc: 0.6520\n",
            "Epoch 42/50\n",
            "81/81 [==============================] - 61s 751ms/step - loss: 0.0104 - acc: 0.8819 - val_loss: 0.1109 - val_acc: 0.6380\n",
            "Epoch 43/50\n",
            "81/81 [==============================] - 60s 738ms/step - loss: 0.0101 - acc: 0.8801 - val_loss: 0.1147 - val_acc: 0.6410\n",
            "Epoch 44/50\n",
            "81/81 [==============================] - 60s 745ms/step - loss: 0.0096 - acc: 0.8840 - val_loss: 0.1122 - val_acc: 0.6330\n",
            "Epoch 45/50\n",
            "81/81 [==============================] - 60s 738ms/step - loss: 0.0092 - acc: 0.8855 - val_loss: 0.1136 - val_acc: 0.6330\n",
            "Epoch 46/50\n",
            "81/81 [==============================] - 59s 732ms/step - loss: 0.0090 - acc: 0.8920 - val_loss: 0.1154 - val_acc: 0.6530\n",
            "Epoch 47/50\n",
            "81/81 [==============================] - 60s 742ms/step - loss: 0.0089 - acc: 0.8921 - val_loss: 0.1164 - val_acc: 0.6330\n",
            "Epoch 48/50\n",
            "81/81 [==============================] - 54s 664ms/step - loss: 0.0086 - acc: 0.8968 - val_loss: 0.1153 - val_acc: 0.6350\n",
            "Epoch 49/50\n",
            "81/81 [==============================] - 50s 617ms/step - loss: 0.0086 - acc: 0.8907 - val_loss: 0.1174 - val_acc: 0.6290\n",
            "Epoch 50/50\n",
            "81/81 [==============================] - 50s 617ms/step - loss: 0.0081 - acc: 0.9024 - val_loss: 0.1182 - val_acc: 0.6230\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: Dimensional_Sentiment_Model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECo5fKvgjE80"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4S1bf4azxes",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b8d01983-deb5-432b-942d-c7bf4b213469"
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/07/1e1da0d87f0cd1e3e3b6fa20ac4ceca43318d9ba10d49206c49dbd2816f2/tensorflowjs-2.0.1.post1-py3-none-any.whl (60kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.19.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.18.5)\n",
            "Collecting tensorflow-cpu<3,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/4f/7bf91c87907873177ad99a31014fb77271a693a3a7cb75e522ac6b556416/tensorflow_cpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (144.4MB)\n",
            "\u001b[K     |████████████████████████████████| 144.4MB 79kB/s \n",
            "\u001b[?25hCollecting PyInquirer==1.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/4c/434b7c454010a284b49d6f1d446fe8dc5960415613d8c0225b9e2efb6724/PyInquirer-1.0.3.tar.gz\n",
            "Collecting tensorflow-hub==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.12.0)\n",
            "Requirement already satisfied: h5py>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.12.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.3.3)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.30.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.9.0)\n",
            "Collecting prompt_toolkit==1.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/3d/b25d35a9f0d381dd1c02d8e04b37c353caaaff4bc32150328eeebe4931f5/prompt_toolkit-1.0.14-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 47.3MB/s \n",
            "\u001b[?25hCollecting Pygments>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/68/106af3ae51daf807e9cdcba6a90e518954eb8b70341cee52995540a53ead/Pygments-2.6.1-py3-none-any.whl (914kB)\n",
            "\u001b[K     |████████████████████████████████| 921kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2016.11.21 in /usr/local/lib/python3.6/dist-packages (from PyInquirer==1.0.3->tensorflowjs) (2019.12.20)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (49.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt_toolkit==1.0.14->PyInquirer==1.0.3->tensorflowjs) (0.2.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-cpu<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Building wheels for collected packages: PyInquirer\n",
            "  Building wheel for PyInquirer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyInquirer: filename=PyInquirer-1.0.3-cp36-none-any.whl size=32851 sha256=2b1a2c58bfc29791308bf5be82dba30f38d201c83f6a65e5219dcda57c5d1d53\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/6c/b1/3e4b0e8daf42a92883c7641c0ea8ffb62e0490ebed2faa55ad\n",
            "Successfully built PyInquirer\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-cpu, prompt-toolkit, Pygments, PyInquirer, tensorflow-hub, tensorflowjs\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: tensorflow-hub 0.8.0\n",
            "    Uninstalling tensorflow-hub-0.8.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.8.0\n",
            "Successfully installed PyInquirer-1.0.3 Pygments-2.6.1 prompt-toolkit-1.0.14 tensorflow-cpu-2.2.0 tensorflow-hub-0.7.0 tensorflowjs-2.0.1.post1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit",
                  "pygments",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQhoXtjFEyP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "4795e346-5ee5-4cd0-88d0-1872f4b191e0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_4 (TimeDist (None, 15, 70, 300)       4682400   \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 15, 68, 32)        28832     \n",
            "_________________________________________________________________\n",
            "time_distributed_6 (TimeDist (None, 15, 67, 32)        0         \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 15, 2144)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20)                173200    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 4,884,664\n",
            "Trainable params: 202,264\n",
            "Non-trainable params: 4,682,400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbtEChKUjKNg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "c89d3f94-ba50-4eb7-b4d6-e90e6b513011"
      },
      "source": [
        "import tensorflowjs as tfjs\n",
        "\n",
        "def get_model_without_embeddings(model, input_shape):\n",
        "  return tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=input_shape),\n",
        "    layers.TimeDistributed(layers.Conv1D(CNN_DIM, CNN_WINDOW, activation='relu', weights=model.get_layer('time_distributed_5').get_weights())),\n",
        "    layers.TimeDistributed(layers.MaxPool1D(pool_size=POOL_SIZE, strides=1, padding='valid', weights=model.get_layer('time_distributed_6').get_weights())),\n",
        "    layers.TimeDistributed(layers.Flatten(weights=model.get_layer('time_distributed_7').get_weights())),\n",
        "    layers.LSTM(20, weights=model.get_layer('lstm_1').get_weights()),\n",
        "    layers.Dense(10, weights=model.get_layer('dense_2').get_weights()),\n",
        "    layers.Dense(2, weights=model.get_layer('dense_3').get_weights())\n",
        "  ])\n",
        "\n",
        "def save_model(path, model):\n",
        "  tfjs.converters.save_keras_model(model, path)\n",
        "  model.save(os.path.join(path, 'model.h5'))\n",
        "\n",
        "embedding_dims=300\n",
        "\n",
        "new_model = get_model_without_embeddings(model, (MAX_ENTRY_LENGTH, MAX_SENTENCE_LENGTH, embedding_dims))\n",
        "new_model.summary()\n",
        "save_model('generated', new_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed_8 (TimeDist (None, 15, 68, 32)        28832     \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, 15, 67, 32)        0         \n",
            "_________________________________________________________________\n",
            "time_distributed_10 (TimeDis (None, 15, 2144)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 20)                173200    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 22        \n",
            "=================================================================\n",
            "Total params: 202,264\n",
            "Trainable params: 202,264\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  return h5py.File(h5file)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}